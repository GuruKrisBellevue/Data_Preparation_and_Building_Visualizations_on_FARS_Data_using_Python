{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fde530a4",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Overview-of-the-Approach-of-this-Project:\" data-toc-modified-id=\"Overview-of-the-Approach-of-this-Project:-0.1\"><strong><em>Overview of the Approach of this Project:</em></strong></a></span></li><li><span><a href=\"#1.-Extracting-Tables\" data-toc-modified-id=\"1.-Extracting-Tables-0.2\">1. Extracting Tables</a></span><ul class=\"toc-item\"><li><span><a href=\"#Format-of-tables-in-the-Source-HTML-of-the-Web-page\" data-toc-modified-id=\"Format-of-tables-in-the-Source-HTML-of-the-Web-page-0.2.1\">Format of tables in the Source HTML of the Web page</a></span></li><li><span><a href=\"#Extracting-Table-1\" data-toc-modified-id=\"Extracting-Table-1-0.2.2\">Extracting Table 1</a></span></li></ul></li><li><span><a href=\"#Extracting-Table-2\" data-toc-modified-id=\"Extracting-Table-2-0.3\">Extracting Table 2</a></span></li><li><span><a href=\"#2.-Extracting-all-tables-in-the-HTML-Source-using-Functions\" data-toc-modified-id=\"2.-Extracting-all-tables-in-the-HTML-Source-using-Functions-0.4\">2. Extracting all tables in the HTML Source using Functions</a></span></li><li><span><a href=\"#3.-Data-Cleansing-and-Transformations\" data-toc-modified-id=\"3.-Data-Cleansing-and-Transformations-0.5\">3. Data Cleansing and Transformations</a></span><ul class=\"toc-item\"><li><span><a href=\"#Transformation-1:-Creating-new-Dataframes-by-selecting-the-required-columns\" data-toc-modified-id=\"Transformation-1:-Creating-new-Dataframes-by-selecting-the-required-columns-0.5.1\">Transformation 1: Creating new Dataframes by selecting the required columns</a></span></li><li><span><a href=\"#Transformation-2:-Renaming-columns\" data-toc-modified-id=\"Transformation-2:-Renaming-columns-0.5.2\">Transformation 2: Renaming columns</a></span></li><li><span><a href=\"#Transformation-3:-Converting-data-to-Upper-case\" data-toc-modified-id=\"Transformation-3:-Converting-data-to-Upper-case-0.5.3\">Transformation 3: Converting data to Upper case</a></span></li><li><span><a href=\"#Transformation-4:-Remove-comma-seperator-and-convert-to-Integer\" data-toc-modified-id=\"Transformation-4:-Remove-comma-seperator-and-convert-to-Integer-0.5.4\">Transformation 4: Remove comma seperator and convert to Integer</a></span></li><li><span><a href=\"#Transformation-5:-Checking-for-Nulls\" data-toc-modified-id=\"Transformation-5:-Checking-for-Nulls-0.5.5\">Transformation 5: Checking for Nulls</a></span></li><li><span><a href=\"#Transformation-6:-Merging-Dataframes-into-1\" data-toc-modified-id=\"Transformation-6:-Merging-Dataframes-into-1-0.5.6\">Transformation 6: Merging Dataframes into 1</a></span></li><li><span><a href=\"#Transformation-7:-Drop-redundant-columns,-Outliers-and-Rename-other-columns-in-Merged-Dataframe\" data-toc-modified-id=\"Transformation-7:-Drop-redundant-columns,-Outliers-and-Rename-other-columns-in-Merged-Dataframe-0.5.7\">Transformation 7: Drop redundant columns, Outliers and Rename other columns in Merged Dataframe</a></span></li><li><span><a href=\"#Transformation-8:-Adding-New-columns\" data-toc-modified-id=\"Transformation-8:-Adding-New-columns-0.5.8\">Transformation 8: Adding New columns</a></span></li><li><span><a href=\"#Transformation-9:-GroupBy-and-Sort-Transformations\" data-toc-modified-id=\"Transformation-9:-GroupBy-and-Sort-Transformations-0.5.9\">Transformation 9: GroupBy and Sort Transformations</a></span></li><li><span><a href=\"#Transformation-10:-Stack-and-Unstack-Transformations\" data-toc-modified-id=\"Transformation-10:-Stack-and-Unstack-Transformations-0.5.10\">Transformation 10: Stack and Unstack Transformations</a></span></li><li><span><a href=\"#Transformation-11:-Pivot-&amp;-Melt-Transformations\" data-toc-modified-id=\"Transformation-11:-Pivot-&amp;-Melt-Transformations-0.5.11\">Transformation 11: Pivot &amp; Melt Transformations</a></span></li></ul></li><li><span><a href=\"#4.-Printing-the-Final-dataframe\" data-toc-modified-id=\"4.-Printing-the-Final-dataframe-0.6\">4. Printing the Final dataframe</a></span></li><li><span><a href=\"#5.Ethical-Implications\" data-toc-modified-id=\"5.Ethical-Implications-0.7\">5.Ethical Implications</a></span></li></ul></li><li><span><a href=\"#\" data-toc-modified-id=\"-1\"></a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87cdb4c",
   "metadata": {},
   "source": [
    "# Cleaning/Formatting Website data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78496f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Beautiful soup\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2d5548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting global options for the notebook such as maxrows\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option(\"display.max_rows\", 50)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7638e5",
   "metadata": {},
   "source": [
    "### ***Overview of the Approach of this Project:*** \n",
    "##### 1. In this project, the data from Website https://www.iihs.org/topics/fatality-statistics/detail/state-by-state#crash-types is sourced , cleansed by applying various transformations which will be used for plotting visualizations(later in the final project)\n",
    "##### 2. There are 6 tables in the website HTML source. Using Beautiful soup for web scraping, data from all 6 tables are extracted and Dataframes are built on the data. As all the tables have similar structure, functions are used to extract data from all 6 tables.\n",
    "##### 3. After the Headings and the data from the tables are extracted, Dataframes are created. \n",
    "##### 4. Out of the 6 tables, 5 tables will be used in this Project. The required columns are selected from the Dataframes.\n",
    "##### 5. The data is then cleansed by removing duplicates, checking for Nulls, Converting to Upper case, etc and finally all the 5 Dataframes are joined together using common columns.\n",
    "##### 6. The data is then joined with the US states data to include additional details such as Region, Division, etc.\n",
    "##### 7. Finally multiple transformations such as GroupBy, Sort, Pivot are applied to the final dataframe which will be used in the final Milestone project for Visualization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff80e2a8",
   "metadata": {},
   "source": [
    "### 1. Extracting Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc245b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting data from the Website using Beautiful Soup\n",
    "url = \"https://www.iihs.org/topics/fatality-statistics/detail/state-by-state#crash-types\"\n",
    "# Using Requests library to pull the Source HTML from the website\n",
    "req = requests.get(url)\n",
    "# Creating Beautiful Soup Object\n",
    "soup = BeautifulSoup(req.content, 'html.parser')\n",
    "# Printing the HTML\n",
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d52bcb",
   "metadata": {},
   "source": [
    "#### Format of tables in the Source HTML of the Web page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7718ba5a",
   "metadata": {},
   "source": [
    "##### 1. There are about 6 tables in the HTML Source. All of them are contained in a div tag and they all share the same class name- \"table fatality-table is-striped is-bordered is-hoverable is-fullwidth\". Hence the div tags with this class name is found and tables within the div tag are searched. \n",
    "##### 2. All the tables have similar structure where the Title and the Header data are contained within the thead section and the table data within the tbody section. \n",
    "##### 3. To extract the table title and Headers, the thead is parsed and the \"th\" within the \"tr\" are then parsed.\n",
    "##### 4. Similarly to extract the table data, the tbody is parsed and the \"td\" within the \"th\" are parsed.\n",
    "##### 5. For the tables 2 to 6, the statistics along with the Percentage information is present in the tables. The Percentage data is skipped while reading the table data.\n",
    "##### 6. As all the 6 tables use similar table structure, resuable function is used to extract the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3498ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the number of html tables in the webpage\n",
    "fatalities_tables=soup.find_all('table')\n",
    "print(f\"There are {len(fatalities_tables)} tables in the Web page\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46af4050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the number of html tables by checking the <div> with the desired class name\n",
    "fatalities_divs=soup.find_all('div', class_=\"table fatality-table is-striped is-bordered is-hoverable is-fullwidth\")\n",
    "# Checking the number of tables using the div \n",
    "print(f\"There are {len(fatalities_divs)} tables in the Web page\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61e045e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting number of table within each <div>\n",
    "for divs in fatalities_divs:\n",
    "    div_tbl=divs.find_all('table')\n",
    "    print(len(div_tbl))\n",
    "# The result confirms that each div tag contains 1 table to be scarped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da527c0",
   "metadata": {},
   "source": [
    "#### Extracting Table 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caf84ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the first table html by searching for table within the first div tag\n",
    "table1=fatalities_divs[0].find('table')\n",
    "table1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c814e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the Table Title within the thead \n",
    "table1_head_tr=table1.thead.find_all('tr')\n",
    "# Title is within the first <tr> tag of thead\n",
    "table1_title=table1_head_tr[0].getText().strip(\"\\n\")\n",
    "print(f\"The Table1 Title is :  {table1_title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd28c760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The table1 headers are within the second <tr> element in the thead. \n",
    "# Within this, each column header is in the <th> element\n",
    "table1_headers=[head_elem1.getText() for head_elem1 in table1_head_tr[1].find_all('th')]\n",
    "print(f\"The Table1 Headers are :  {table1_headers}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca37535a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the table data by searching the tbody and finding the <tr> element\n",
    "table1_data_tr=table1.tbody.find_all('tr')\n",
    "# Within each <tr> element, the table data is in both <td> and <th> elements, hence extracting both \n",
    "table1_data=[[data_elem2.getText() for data_elem2 in table1_data_tr[itera].find_all(['td','th'])] for itera in range(len(table1_data_tr)) ]   \n",
    "table1_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e56c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the first table dataframe from the header and data extracted\n",
    "table1_df=pd.DataFrame(table1_data,columns=table1_headers)\n",
    "\n",
    "print(f\"The Table1 dataframe is : {table1_title}\")\n",
    "display(table1_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41857268",
   "metadata": {},
   "source": [
    "### Extracting Table 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffd11d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the Second table html by searching for table within the second div tag\n",
    "table2=fatalities_divs[1].find('table')\n",
    "table2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44cc9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the Table Title within the thead \n",
    "table2_head_tr=table2.thead.find_all('tr')\n",
    "# Title is within the first <tr> tag of thead\n",
    "table2_title=table2_head_tr[0].getText().strip(\"\\n\")\n",
    "print(f\"The Table2 Title is :  {table2_title}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348e3496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The table2 headers are within the second <tr> element in the thead. \n",
    "# Within this, each column header is in the <th> element\n",
    "table2_headers=[head_elem1.getText() for head_elem1 in table2_head_tr[1].find_all('th')]\n",
    "print(f\"The Table2 Headers are :  {table2_headers}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc67e622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the table data by searching the tbody and finding the <tr> element\n",
    "table2_data_tr=table2.tbody.find_all('tr')\n",
    "# Within each <tr> element, the table data is in both <td> and <th> elements, hence extracting both \n",
    "table2_data=[[data_elem2.getText() for data_elem2 in table2_data_tr[itera].find_all(['td','th'])] for itera in range(len(table2_data_tr)) ]   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774771cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Table2 data contains the number and percentage values. As we are only interested in the number,\n",
    "# Skipping the percent values, by skipping the values at even index with the exception of 0 index.\n",
    "table2_data_mod=[ [item for index,item in enumerate(table2_data[iterat]) if ( (index==0) | (index%2==1) )] for iterat in range(len(table2_data))]\n",
    "# Printing top 4 elements of Modified Table2 data.\n",
    "table2_data_mod[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b925630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the second table dataframe from the header and data extracted\n",
    "table2_df=pd.DataFrame(table2_data_mod,columns=table2_headers)\n",
    "print(f\"The Table2 dataframe is : {table2_title}\")\n",
    "display(table2_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4235bccf",
   "metadata": {},
   "source": [
    "### 2. Extracting all tables in the HTML Source using Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4f2eee",
   "metadata": {},
   "source": [
    "##### As all the tables have similar structure as Table1 and Table2, using functions to extract the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ce20fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table(table_data, skip=0):\n",
    "    \"\"\"\n",
    "    This function returns the modified table data based on the skip indicator value passed. \n",
    "    For the first table, the skip value is 0 so the data is unmodified. For all other tables, the\n",
    "    percentage values are skipped by passing the skip indicator value as 1.\n",
    "    \"\"\"\n",
    "    # Creating a new variable to use in the Mod function\n",
    "    modulo=skip+1\n",
    "    # This will return modified table by skipping the even values(with the exception of 0 index)\n",
    "    # If the Skip value is 0, the returned data is unmodified.\n",
    "    #If the skip value is 1, the returned dataframe skips the values at even indexes.\n",
    "    table_data_mod=[ [item for index,item in enumerate(table_data[iterat]) if ( (index==0) | (index%modulo==skip) )] for iterat in range(len(table_data))]\n",
    "    return table_data_mod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0a6965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_DataFrame(final_table,table_headers):\n",
    "    \"\"\"\n",
    "    This function will create final Dataframe based on the Table data and the header data passed as arguments\n",
    "    \"\"\"\n",
    "    table_df=pd.DataFrame(final_table,columns=table_headers)\n",
    "    return table_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0675cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_HTML(index2):\n",
    "    \"\"\"\n",
    "    This function will create the Final Dataframe by extracting the table data from Source HTML. \n",
    "    It takes the table number/index as an input and returns the dataframe. \n",
    "    For instance to extract the first table, the index passed will be 0.\n",
    "    \"\"\"\n",
    "    # Parsing each div object created in the previous step\n",
    "    for index1,item1 in enumerate(fatalities_divs):\n",
    "        # By comparing the index in div object and the index passed in the function, \n",
    "        # we are making sure, only the desired table is created for the specified index.\n",
    "        if index1==index2:\n",
    "            # Searching for 'table' element in the div object\n",
    "            table=fatalities_divs[index1].find('table')\n",
    "            # Searching for tr element in the thead\n",
    "            table_head_tr=table.thead.find_all('tr')\n",
    "            # Table Title is within the first <tr> tag of thead\n",
    "            table_title=table_head_tr[0].getText().strip(\"\\n\")\n",
    "            # The table2 headers are within the second <tr> element in the thead.\n",
    "            # within this, each column header is in the <th> element\n",
    "            table_headers=[head_elem1.getText() for head_elem1 in table_head_tr[1].find_all('th')]\n",
    "            # Extracting the table data by searching the tbody and finding the <tr> element\n",
    "            table_data_tr=table.tbody.find_all('tr')\n",
    "            # Within each <tr> element, the table data is in both <td> and <th> elements, hence extracting both \n",
    "            table_data=[[data_elem2.getText() for data_elem2 in table_data_tr[itera].find_all(['td','th'])] for itera in range(len(table_data_tr)) ]   \n",
    "            # As some tables have percent and Number values, creating skip variable for those tables \n",
    "            # to skip the percentage elements. For the first table there is no skip and for all other table we skip the percent values\n",
    "            skip=0 if index1==0 else 1\n",
    "            \"\"\"\n",
    "            if index1==0:\n",
    "                skip=0\n",
    "            else:\n",
    "                skip=1\n",
    "                \"\"\"\n",
    "            # Calling the create table function to create modified table based on skip values passed\n",
    "            final_table=create_table(table_data, skip)\n",
    "            # Creating final dataframe based on the Table data and Header extracted\n",
    "            final_df=create_DataFrame(final_table,table_headers)\n",
    "        else:\n",
    "            # If the specified index does not match, control is passed to the next index in the loop\n",
    "            continue\n",
    "        # Final dataframe is returned\n",
    "        return final_df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9713f2",
   "metadata": {},
   "source": [
    "##### Creating tables using dynamic variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5ef6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the number of tables in the <div> elements\n",
    "nbr_tables=len(fatalities_divs)\n",
    "# Looping through each div element to create the dataframe by parsing the HTML\n",
    "for index2 in range(nbr_tables):\n",
    "    # Creating tables using dynamic variable in the format table_df_<index number> starting from 0 through 5\n",
    "    globals()[f\"table_df_{index2}\"]=parse_HTML(index2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a89f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the results by checking the tables extracted\n",
    "table_df_0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ff249d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting top 5 elements of table 2\n",
    "table_df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57db3f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting top 5 elements of table 3\n",
    "table_df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657b2ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting top 5 elements of table 4\n",
    "table_df_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bcdf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting top 5 elements of table 6. \n",
    "table_df_5.head()\n",
    "# Note: Table 5 is not displayed, as we won't be using that data for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2041fb7",
   "metadata": {},
   "source": [
    "### 3. Data Cleansing and Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61071df1",
   "metadata": {},
   "source": [
    "#### Transformation 1: Creating new Dataframes by selecting the required columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec22b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating 5 new dataframes with the required columns\n",
    "fatal_crash_totals_df=table_df_0[[\"State\",\"Population\",\"Fatal crashes\",\"Deaths\"]]\n",
    "deaths_by_roaduser_df=table_df_1\n",
    "crash_types_df=table_df_2\n",
    "alc_df=table_df_3[[\"State\",\"Total drivers killed\"]]\n",
    "rural_urban_df=table_df_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9266d942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning Name attribute for the Dataframes\n",
    "fatal_crash_totals_df.name='fatal_crash_totals_df'\n",
    "deaths_by_roaduser_df.name='deaths_by_roaduser_df'\n",
    "crash_types_df.name='crash_types_df'\n",
    "alc_df.name='alc_df'\n",
    "rural_urban_df.name='rural_urban_df'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff035c2",
   "metadata": {},
   "source": [
    "#### Transformation 2: Renaming columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ebbead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns in fatal_crash_totals_df and doing an inplace update\n",
    "fatal_crash_totals_df.rename(columns={\"Fatal crashes\":\"Fatal_crashes\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d70718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns in deaths_by_roaduser_df and doing an inplace update\n",
    "deaths_by_roaduser_df.rename(columns={\"Car occupants\":\"Car_occup\",\n",
    "                                      \"Pickup and SUV occupants\":\"Pickup_SUV_occup\",\n",
    "                                      \"Large truck occupants\":\"Truck_occup\",\n",
    "                                      \"Total*\":\"Total\"\n",
    "                                     },inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf927a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns in deaths_by_roaduser_df and doing an inplace update\n",
    "crash_types_df.rename(columns={\"Single-vehicle\":\"Single_vehicle\",\n",
    "                                      \"Multiple-vehicle\":\"Multiple_vehicle\",\n",
    "                                      \"All crashes\":\"All_crashes\"\n",
    "                                      },inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de3de0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns in alcohol df and doing an inplace update\n",
    "alc_df.rename(columns={\"Total drivers killed\":\"Total_drivers_killed\"},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55efc65",
   "metadata": {},
   "source": [
    "#### Transformation 3: Converting data to Upper case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826006f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toUpper(df,col):\n",
    "    \"\"\"\n",
    "    This fnction converts all the values in the given column of dataframe to Upper case and removes extra spaces from the string..\n",
    "    Inputs: Dataframe name and Column Name.\n",
    "    Performs an inplace update by converting to Upper case and removing extra spaces.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Using apply method to convert to UpperCase and strip spaces.\n",
    "        df[col]=df[col].apply(str.strip)\n",
    "        df[col]=df[col].apply(str.upper)\n",
    "    except TypeError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4cf6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating list of Dataframes, String columns from each Dataframe\n",
    "all_df_list=[fatal_crash_totals_df,deaths_by_roaduser_df,crash_types_df,alc_df,rural_urban_df]\n",
    "string_cols_list=[[\"State\"],[\"State\"],[\"State\"],[\"State\"],[\"State\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b9edce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping through the list to convert the columns in string_cols_list from each Dataframe using the Index\n",
    "for index3,df_name in enumerate(all_df_list):\n",
    "    # Comparing based on index of the all_df_list and extracting the column from string_cols_list based on the index\n",
    "    for col in string_cols_list[index3]:\n",
    "        # converting the column data to Upper case\n",
    "        toUpper(df_name,col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da03562",
   "metadata": {},
   "source": [
    "#### Transformation 4: Remove comma seperator and convert to Integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35ca502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanse_int_cols(df,col):\n",
    "    \"\"\"\n",
    "    This function will do an inplace update by removing comma. This is used to convert the columns that\n",
    "    has Numeric data that are stored as Strings.\n",
    "    It takes Dataframe name and column as inputs and returns a clean dataframe.\n",
    "    \"\"\"\n",
    "    df[col]=df[col].str.replace(\",\",\"\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249539e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list  of Integer columns\n",
    "int_cols_list=[[\"Population\",\"Fatal_crashes\",\"Deaths\"],[\"Car_occup\",\"Pickup_SUV_occup\",\"Truck_occup\",\"Motorcyclists\",\"Pedestrians\",\"Bicyclists\",\"Total\"],\n",
    "               [\"Single_vehicle\",\"Multiple_vehicle\",\"All_crashes\"],\n",
    "               [\"Total_drivers_killed\"],\n",
    "               [\"Urban\",\"Rural\",\"Unknown\",\"Total\"]\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d86e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping through the list to convert the columns in int_cols_list from each Dataframe using the Index values\n",
    "for index4,df_name in enumerate(all_df_list):\n",
    "    # Comparing based on index of the all_df_list and extracting the column from int_cols_list based on the index\n",
    "    for col in int_cols_list[index4]:\n",
    "        # calling Function to convert the string values to integer by removing commas in the number.\n",
    "        cleanse_int_cols(df_name,col)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbf8491",
   "metadata": {},
   "source": [
    "#### Transformation 5: Checking for Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce535b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_NaN(df,col):\n",
    "    \"\"\"\n",
    "    This function checks the number of nulls in a columnof dataframe.\n",
    "    Input: Takes the name of dataframe and the column name\n",
    "    Returns: The count of nulls in the column\n",
    "    \"\"\"\n",
    "    col_nan=df[col].isnull().sum()\n",
    "    return col_nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db558cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_NaN(df,col,fill_value=0):\n",
    "    \"\"\"\n",
    "    This function calls the check_NaN to verify the presence of Nulls. If present, they are filled with  filler.\n",
    "    Input: Takes, Dataframe name, Column name and the Filler value as Inputs.\n",
    "    Performs an inplace update of filling Nulls with Filler.\n",
    "    Also Prints the number of nulls present in the column.    \n",
    "    \"\"\"\n",
    "    # Calling the check_NaN function to verify if nulls are present\n",
    "    col_nan= check_NaN(df,col)\n",
    "    # If Nulls are present, they are replaced with Fillers\n",
    "    if col_nan >0:\n",
    "        print(f\"There are {col_nan} NaN values in the {col} column \")\n",
    "        # Inplace update is performed by replacing NaN with filler and results are printed\n",
    "        df[col].fillna(fill_value,inplace=True)\n",
    "        print(f\"Nulls are replaced with {fill_value} in the {col} column \")\n",
    "    # If nulls are not present, the result is just printed.\n",
    "    else:\n",
    "        print(f\"There are NO NaN values in the {col} column \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64f9c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for Null values in all 5 dataframes by looping through the list\n",
    "for df in all_df_list:\n",
    "    print(\"**\"*50)\n",
    "    # Displaying the name of the Dataframe\n",
    "    print(f\"Checking for Nulls in {df.name}\")\n",
    "    # Looping through each column in the Dataframe\n",
    "    for col in df.columns:\n",
    "        fill_NaN(df,col,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e716d8",
   "metadata": {},
   "source": [
    "#### Transformation 6: Merging Dataframes into 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaecf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging Dataframes 1, 2 and 3 into a temp df\n",
    "merged_temp_df1=fatal_crash_totals_df.merge(deaths_by_roaduser_df,on=['State'],how='inner').merge(crash_types_df,on=['State'],how='inner')\n",
    "# Merging temp df with Dataframes 4 and 5\n",
    "merged_fatal_df=merged_temp_df1.merge(alc_df,on=['State'],how='inner').merge(rural_urban_df,on=['State'],how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0963f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing top 5 rows of Merged Dataframe\n",
    "merged_fatal_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b4e069",
   "metadata": {},
   "source": [
    "#### Transformation 7: Drop redundant columns, Outliers and Rename other columns in Merged Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb21f97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the duplicate columns that represent the same data such as as 'Total Deaths'\n",
    "# Unknown values are considered Outliers and they are excluded\n",
    "merged_fatal_df=merged_fatal_df.drop(['Total_x','All_crashes','Total_y','Unknown'],axis=1)\n",
    "# Renaming columns in the Merged Dataframe\n",
    "merged_fatal_df.rename(columns={\"Deaths\":\"Total_Fatalities\",\n",
    "                                    \"Car_occup\":\"Car_Fatals\",\n",
    "                                    \"Pickup_SUV_occup\":\"SUV_Fatals\",\n",
    "                                    \"Truck_occup\":\"Truck_Fatals\",\n",
    "                                    \"Motorcyclists\":\"Motorcyclist_Fatals\",\n",
    "                                    \"Pedestrians\":\"Pedestrian_Fatals\",\n",
    "                                    \"Bicyclists\":\"Bicyclist_Fatals\",\n",
    "                                    \"Single-vehicle\":\"Single_Vehicle_Fatals\",\n",
    "                                    \"Multiple-vehicle\":\"Multiple_Vehicle_Fatals\",\n",
    "                                    \"Total_drivers_killed\":\"Alcohol_Related_Fatals\",\n",
    "                                    \"Urban\":\"Urban_Fatals\",\n",
    "                                    \"Rural\":\"Rural_Fatals\"\n",
    "                                      },inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cbeb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing top 5 rows of Merged Dataframe\n",
    "merged_fatal_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066da221",
   "metadata": {},
   "source": [
    "#### Transformation 8: Adding New columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ab903d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the data about 50 US states and DC including the State code, Region and Division from dictionary\n",
    "US_States_dict={'State': {'Alaska': 'Alaska',\n",
    "  'Alabama': 'Alabama',\n",
    "  'Arkansas': 'Arkansas',\n",
    "  'Arizona': 'Arizona',\n",
    "  'California': 'California',\n",
    "  'Colorado': 'Colorado',\n",
    "  'Connecticut': 'Connecticut',\n",
    "  'District of Columbia': 'District of Columbia',\n",
    "  'Delaware': 'Delaware',\n",
    "  'Florida': 'Florida',\n",
    "  'Georgia': 'Georgia',\n",
    "  'Hawaii': 'Hawaii',\n",
    "  'Iowa': 'Iowa',\n",
    "  'Idaho': 'Idaho',\n",
    "  'Illinois': 'Illinois',\n",
    "  'Indiana': 'Indiana',\n",
    "  'Kansas': 'Kansas',\n",
    "  'Kentucky': 'Kentucky',\n",
    "  'Louisiana': 'Louisiana',\n",
    "  'Massachusetts': 'Massachusetts',\n",
    "  'Maryland': 'Maryland',\n",
    "  'Maine': 'Maine',\n",
    "  'Michigan': 'Michigan',\n",
    "  'Minnesota': 'Minnesota',\n",
    "  'Missouri': 'Missouri',\n",
    "  'Mississippi': 'Mississippi',\n",
    "  'Montana': 'Montana',\n",
    "  'North Carolina': 'North Carolina',\n",
    "  'North Dakota': 'North Dakota',\n",
    "  'Nebraska': 'Nebraska',\n",
    "  'New Hampshire': 'New Hampshire',\n",
    "  'New Jersey': 'New Jersey',\n",
    "  'New Mexico': 'New Mexico',\n",
    "  'Nevada': 'Nevada',\n",
    "  'New York': 'New York',\n",
    "  'Ohio': 'Ohio',\n",
    "  'Oklahoma': 'Oklahoma',\n",
    "  'Oregon': 'Oregon',\n",
    "  'Pennsylvania': 'Pennsylvania',\n",
    "  'Rhode Island': 'Rhode Island',\n",
    "  'South Carolina': 'South Carolina',\n",
    "  'South Dakota': 'South Dakota',\n",
    "  'Tennessee': 'Tennessee',\n",
    "  'Texas': 'Texas',\n",
    "  'Utah': 'Utah',\n",
    "  'Virginia': 'Virginia',\n",
    "  'Vermont': 'Vermont',\n",
    "  'Washington': 'Washington',\n",
    "  'Wisconsin': 'Wisconsin',\n",
    "  'West Virginia': 'West Virginia',\n",
    "  'Wyoming': 'Wyoming'},\n",
    " 'State Code': {'Alaska': 'AK',\n",
    "  'Alabama': 'AL',\n",
    "  'Arkansas': 'AR',\n",
    "  'Arizona': 'AZ',\n",
    "  'California': 'CA',\n",
    "  'Colorado': 'CO',\n",
    "  'Connecticut': 'CT',\n",
    "  'District of Columbia': 'DC',\n",
    "  'Delaware': 'DE',\n",
    "  'Florida': 'FL',\n",
    "  'Georgia': 'GA',\n",
    "  'Hawaii': 'HI',\n",
    "  'Iowa': 'IA',\n",
    "  'Idaho': 'ID',\n",
    "  'Illinois': 'IL',\n",
    "  'Indiana': 'IN',\n",
    "  'Kansas': 'KS',\n",
    "  'Kentucky': 'KY',\n",
    "  'Louisiana': 'LA',\n",
    "  'Massachusetts': 'MA',\n",
    "  'Maryland': 'MD',\n",
    "  'Maine': 'ME',\n",
    "  'Michigan': 'MI',\n",
    "  'Minnesota': 'MN',\n",
    "  'Missouri': 'MO',\n",
    "  'Mississippi': 'MS',\n",
    "  'Montana': 'MT',\n",
    "  'North Carolina': 'NC',\n",
    "  'North Dakota': 'ND',\n",
    "  'Nebraska': 'NE',\n",
    "  'New Hampshire': 'NH',\n",
    "  'New Jersey': 'NJ',\n",
    "  'New Mexico': 'NM',\n",
    "  'Nevada': 'NV',\n",
    "  'New York': 'NY',\n",
    "  'Ohio': 'OH',\n",
    "  'Oklahoma': 'OK',\n",
    "  'Oregon': 'OR',\n",
    "  'Pennsylvania': 'PA',\n",
    "  'Rhode Island': 'RI',\n",
    "  'South Carolina': 'SC',\n",
    "  'South Dakota': 'SD',\n",
    "  'Tennessee': 'TN',\n",
    "  'Texas': 'TX',\n",
    "  'Utah': 'UT',\n",
    "  'Virginia': 'VA',\n",
    "  'Vermont': 'VT',\n",
    "  'Washington': 'WA',\n",
    "  'Wisconsin': 'WI',\n",
    "  'West Virginia': 'WV',\n",
    "  'Wyoming': 'WY'},\n",
    " 'Region': {'Alaska': 'West',\n",
    "  'Alabama': 'South',\n",
    "  'Arkansas': 'South',\n",
    "  'Arizona': 'West',\n",
    "  'California': 'West',\n",
    "  'Colorado': 'West',\n",
    "  'Connecticut': 'Northeast',\n",
    "  'District of Columbia': 'South',\n",
    "  'Delaware': 'South',\n",
    "  'Florida': 'South',\n",
    "  'Georgia': 'South',\n",
    "  'Hawaii': 'West',\n",
    "  'Iowa': 'Midwest',\n",
    "  'Idaho': 'West',\n",
    "  'Illinois': 'Midwest',\n",
    "  'Indiana': 'Midwest',\n",
    "  'Kansas': 'Midwest',\n",
    "  'Kentucky': 'South',\n",
    "  'Louisiana': 'South',\n",
    "  'Massachusetts': 'Northeast',\n",
    "  'Maryland': 'South',\n",
    "  'Maine': 'Northeast',\n",
    "  'Michigan': 'Midwest',\n",
    "  'Minnesota': 'Midwest',\n",
    "  'Missouri': 'Midwest',\n",
    "  'Mississippi': 'South',\n",
    "  'Montana': 'West',\n",
    "  'North Carolina': 'South',\n",
    "  'North Dakota': 'Midwest',\n",
    "  'Nebraska': 'Midwest',\n",
    "  'New Hampshire': 'Northeast',\n",
    "  'New Jersey': 'Northeast',\n",
    "  'New Mexico': 'West',\n",
    "  'Nevada': 'West',\n",
    "  'New York': 'Northeast',\n",
    "  'Ohio': 'Midwest',\n",
    "  'Oklahoma': 'South',\n",
    "  'Oregon': 'West',\n",
    "  'Pennsylvania': 'Northeast',\n",
    "  'Rhode Island': 'Northeast',\n",
    "  'South Carolina': 'South',\n",
    "  'South Dakota': 'Midwest',\n",
    "  'Tennessee': 'South',\n",
    "  'Texas': 'South',\n",
    "  'Utah': 'West',\n",
    "  'Virginia': 'South',\n",
    "  'Vermont': 'Northeast',\n",
    "  'Washington': 'West',\n",
    "  'Wisconsin': 'Midwest',\n",
    "  'West Virginia': 'South',\n",
    "  'Wyoming': 'West'},\n",
    " 'Division': {'Alaska': 'Pacific',\n",
    "  'Alabama': 'East South Central',\n",
    "  'Arkansas': 'West South Central',\n",
    "  'Arizona': 'Mountain',\n",
    "  'California': 'Pacific',\n",
    "  'Colorado': 'Mountain',\n",
    "  'Connecticut': 'New England',\n",
    "  'District of Columbia': 'South Atlantic',\n",
    "  'Delaware': 'South Atlantic',\n",
    "  'Florida': 'South Atlantic',\n",
    "  'Georgia': 'South Atlantic',\n",
    "  'Hawaii': 'Pacific',\n",
    "  'Iowa': 'West North Central',\n",
    "  'Idaho': 'Mountain',\n",
    "  'Illinois': 'East North Central',\n",
    "  'Indiana': 'East North Central',\n",
    "  'Kansas': 'West North Central',\n",
    "  'Kentucky': 'East South Central',\n",
    "  'Louisiana': 'West South Central',\n",
    "  'Massachusetts': 'New England',\n",
    "  'Maryland': 'South Atlantic',\n",
    "  'Maine': 'New England',\n",
    "  'Michigan': 'East North Central',\n",
    "  'Minnesota': 'West North Central',\n",
    "  'Missouri': 'West North Central',\n",
    "  'Mississippi': 'East South Central',\n",
    "  'Montana': 'Mountain',\n",
    "  'North Carolina': 'South Atlantic',\n",
    "  'North Dakota': 'West North Central',\n",
    "  'Nebraska': 'West North Central',\n",
    "  'New Hampshire': 'New England',\n",
    "  'New Jersey': 'Middle Atlantic',\n",
    "  'New Mexico': 'Mountain',\n",
    "  'Nevada': 'Mountain',\n",
    "  'New York': 'Middle Atlantic',\n",
    "  'Ohio': 'East North Central',\n",
    "  'Oklahoma': 'West South Central',\n",
    "  'Oregon': 'Pacific',\n",
    "  'Pennsylvania': 'Middle Atlantic',\n",
    "  'Rhode Island': 'New England',\n",
    "  'South Carolina': 'South Atlantic',\n",
    "  'South Dakota': 'West North Central',\n",
    "  'Tennessee': 'East South Central',\n",
    "  'Texas': 'West South Central',\n",
    "  'Utah': 'Mountain',\n",
    "  'Virginia': 'South Atlantic',\n",
    "  'Vermont': 'New England',\n",
    "  'Washington': 'Pacific',\n",
    "  'Wisconsin': 'East North Central',\n",
    "  'West Virginia': 'South Atlantic',\n",
    "  'Wyoming': 'Mountain'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb36afc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframe from the US states dictionary\n",
    "US_State_Region_df=pd.DataFrame(US_States_dict)\n",
    "US_State_Region_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92f2aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the data in US states Dataframe to Upper Case\n",
    "for col in US_State_Region_df.columns:\n",
    "    toUpper(US_State_Region_df,col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77df98e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing a Left join between merged dataframe and US Stataes DF to add columns such as Region, Devision, State code\n",
    "final_fatals_df=merged_fatal_df.merge(US_State_Region_df,on=['State'],how='left')\n",
    "# Converting the Column names of final dataframe to Upper Case\n",
    "final_fatals_df.columns=final_fatals_df.columns.str.upper()\n",
    "# Renaming column\n",
    "final_fatals_df.rename(columns={\"STATE CODE\":\"STATE_CODE\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94122f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treating for Nulls in the final dataframe\n",
    "for col in final_fatals_df.columns:\n",
    "    fill_NaN(final_fatals_df,col,'NA')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea26ca2",
   "metadata": {},
   "source": [
    "#### Transformation 9: GroupBy and Sort Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ab6635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe of 50 states and the DC excluding the US Totals\n",
    "fatals_USStates_df=final_fatals_df[~final_fatals_df['STATE_CODE'].str.contains('NA')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504157b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping the Datafarme based on Division and calculating the sum of Fatalities in each division. \n",
    "# Results are ordered in descending order\n",
    "fatals_byDivision_df=fatals_USStates_df.groupby([\"DIVISION\"],sort=True)[\"TOTAL_FATALITIES\"].sum().sort_values(ascending=False)\n",
    "fatals_byDivision_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a697ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping the Dataframe based on Region and getting the sum of Alcohol related deaths in each region\n",
    "# Results are ordered in descending order\n",
    "alc_fatals_byRegion=fatals_USStates_df.groupby([\"REGION\"],sort=True)[\"ALCOHOL_RELATED_FATALS\"].sum().sort_values(ascending=False)\n",
    "alc_fatals_byRegion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1dec65",
   "metadata": {},
   "source": [
    "#### Transformation 10: Stack and Unstack Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536ee658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Dataframe subset and sortinng results based on Region name and Fatalities in Urban\n",
    "urban_rural_df1=fatals_USStates_df[[\"STATE\",\"URBAN_FATALS\",\"RURAL_FATALS\",\"REGION\"]].sort_values(by=['REGION',\"URBAN_FATALS\"],ascending=False)\n",
    "# Selecting only Midwest Region\n",
    "urban_rural_df1=urban_rural_df1[urban_rural_df1[\"REGION\"].isin(['MIDWEST'])]\n",
    "# Stting State name as Index\n",
    "urban_rural_df1.set_index(\"STATE\",inplace=True)\n",
    "urban_rural_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb001817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling Unstack method to stack the Region, Urban and Rural fatalities based on State name\n",
    "display(urban_rural_df1.unstack())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c1fedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reindexing the dataframe and giving a name for the columns to be used in the next step\n",
    "urban_rural_df1=urban_rural_df1.reindex(columns=[\"URBAN_FATALS\",\"RURAL_FATALS\",\"REGION\"])\n",
    "urban_rural_df1.columns.name=\"FATAL_STATS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8f35bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Stack operation to show the results grouped by each State\n",
    "urban_rural_expand_df1=urban_rural_df1.stack().reset_index().rename(columns={0:\"VALUE\"})\n",
    "urban_rural_expand_df1.head(21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f2df6f",
   "metadata": {},
   "source": [
    "#### Transformation 11: Pivot & Melt Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69813fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Pivot operation on the Stacked dataframe to restore the original data\n",
    "urban_rural_pivot_df1=urban_rural_expand_df1.pivot(index=\"STATE\",columns=\"FATAL_STATS\",values=\"VALUE\")\n",
    "urban_rural_pivot_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e397dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new Subset dataframe to be used for Melt function\n",
    "urban_rural_df2=fatals_USStates_df[[\"STATE\",\"URBAN_FATALS\",\"RURAL_FATALS\",\"DIVISION\"]].sort_values(by=['DIVISION',\"URBAN_FATALS\"],ascending=False)\n",
    "# Using the Mountain Division data only\n",
    "urban_rural_df2=urban_rural_df2[urban_rural_df2[\"DIVISION\"].isin(['MOUNTAIN'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7de1a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the Melt function to group the data based on State and display the Division name,\n",
    "# Urban and Rural fatalities in that state. This is similar to the stack operation done in above step.\n",
    "urban_rural_melt_df2=pd.melt(urban_rural_df2,id_vars=\"STATE\").sort_values(by=['STATE'])\n",
    "urban_rural_melt_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f142e210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Pivot on the above Melt step to restore the original data\n",
    "urban_rural_pivot_df2=urban_rural_melt_df2.pivot(index=\"STATE\",columns=\"variable\",values=\"value\")\n",
    "urban_rural_pivot_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73747dd",
   "metadata": {},
   "source": [
    "### 4. Printing the Final dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfffce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(fatals_USStates_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a86d10",
   "metadata": {},
   "source": [
    "### 5.Ethical Implications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971c1efa",
   "metadata": {},
   "source": [
    "##### a. While reporting the Alcohol involvement, not all data is reported. Some states have a high reporting rate such as Hawaii, while some states like Mississippi have very low reporting rate. This may lead to inconsitencies while deriving metrics and conclusions from the data.\n",
    "##### b. While extracting the Rural and Urban data from the table , the  \"Unknown\" category was excluded from this analysis  and was considered an Outlier. Care must be taken while reporting the results based on this category alone. For instance, in Mississippi there were about 79 Unknown cases which was rougly 10% of the data. Hence, such Unknwn data may lead to misleading results.\n",
    "##### c. The data about the seat belt use(Restraint use) was not included in this study as there were many unknown values in this category for many states. However some states such as California had the highest percent of seat belt use while New-Hamshire had the lowest. Hence the data from this table can be useful while performing Analysis on certain states where we have good amount of data. Hence though this data was not considered significant for this project, it can be useful while analysing the fatalities data for certain states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4271d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fatals_USStates_df.to_excel('formatted_webdata_df.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e25fc27",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
